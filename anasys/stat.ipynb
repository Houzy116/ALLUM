{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from tool import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data\n",
    "def get_stat(sza_L_mode,band_mode,landcover_type):\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15  \n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    if band_mode=='shortwave':  \n",
    "        WB_sky_fraction=torch.load(\"/data/hk/albedo/white_sky_fraction/white_sky_fraction2.pth\")\n",
    "    else:\n",
    "        if band_mode not in ['nir','vis']:\n",
    "            raise('Error band_mode!')\n",
    "        WB_sky_fraction=torch.load(f\"/data/hk/albedo/white_sky_fraction/white_sky_fraction_{band_mode}2.pth\")\n",
    "    landtypes=['landtype'+str(i) for i in range(1,lc_len+1)]+['snow']\n",
    "    stat={}\n",
    "    for month in range(1,13):\n",
    "        NC=nc.Dataset(f\"/data2/hzy/albedo2/albedo_information_nc2/fill/month_{month}_{landcover_type}.nc\",'r')\n",
    "        for year in range(2001,2021):\n",
    "            types_stat_albedo=[]\n",
    "            types_stat_area=[]\n",
    "            for type in landtypes:\n",
    "                bs=NC.variables[f'albedo_BSA_{band_mode}-{type}'][year-2001]\n",
    "                ws=NC.variables[f'albedo_WSA_{band_mode}-{type}'][year-2001]\n",
    "                area_type=NC.variables[f'area-{type}'][year-2001] \n",
    "                area_land=NC.variables[f'area-land'][year-2001] \n",
    "                wf=WB_sky_fraction[(year-2001)*12+month-1]\n",
    "                if sza_L_mode=='70':\n",
    "                    area_type[sza_L[year-2001,month-1]>=1]=np.nan###\n",
    "                elif sza_L_mode=='85':\n",
    "                    area_type[sza_L[year-2001,month-1]>=2]=np.nan###\n",
    "                else:\n",
    "                    raise('Error sza_L_mode')\n",
    "                types_stat_albedo.append(((ws*wf+bs*(1-wf))/1000))      \n",
    "                types_stat_area.append(area_type)\n",
    "            types_stat_albedo=np.stack(types_stat_albedo)\n",
    "            types_stat_area=np.stack(types_stat_area)\n",
    "            types_stat_albedo[types_stat_albedo==0]=np.nan\n",
    "            stat[f'{year}-{month}-albedo']=types_stat_albedo\n",
    "            stat[f'{year}-{month}-typearea']=types_stat_area\n",
    "            stat[f'{year}-landarea']=area_land   \n",
    "        NC.close() \n",
    "    save_path=f'/data2/hzy/albedo2/data/stat_sw-mean_sza-{sza_L_mode}_band-{band_mode}_{landcover_type}.pth'\n",
    "    print(save_path)\n",
    "    torch.save(stat,save_path)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_LCCS1.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_LCCS1.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_LCCS2.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_LCCS2.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_LCCS3.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_LCCS3.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_IGBP.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_IGBP.pth\n"
     ]
    }
   ],
   "source": [
    "# Organize data\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    for sza_L_mode in ['70','85']:\n",
    "        for band_mode in ['shortwave']:\n",
    "            get_stat(sza_L_mode,band_mode,landcover_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the albedo change curve and seasonal variance.\n",
    "def get_01_season(s,e,landcover_type,mode='region',snowfree=True):\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kk=[[],[],[],[]]\n",
    "    data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_2020_{landcover_type}.pth\")\n",
    "    mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "    data_m[mask_t>=1]=np.nan\n",
    "    data_m=month_fill(data_m,mask_t>=1)\n",
    "    if snowfree:\n",
    "        data_m[:,:,:,-1,:]=0\n",
    "        data_m[:,:,:,:,-1]=0\n",
    "    z=albedo_all[:,0].transpose((0,2,3,1)).reshape((12,180,360,lc_len+1,1))*data_m\n",
    "    n=0\n",
    "    for month in [[3,4,5],[6,7,8],[9,10,11],[12,1,2]]:\n",
    "        if mode=='all':\n",
    "            tt=[z[i-1,:] for i in month]\n",
    "        else:\n",
    "            tt=[z[i-1,s:e] for i in month]\n",
    "        mts=np.array([mt[i-1] for i in month]).reshape(3,1,1,1,1)\n",
    "        tt=np.stack(tt)*mts/np.sum(mts)\n",
    "        if mode=='all':\n",
    "            mm_area=np.array([np.nansum(data_m[i-1,:]) for i in month]).mean()\n",
    "        else:\n",
    "            mm_area=np.array([np.nansum(data_m[i-1,s:e]) for i in month]).mean()\n",
    "        kk[n].append(np.nansum(tt)/mm_area)\n",
    "        n+=1\n",
    "    return kk\n",
    "def get_01_all(s,e,landcover_type,mode='region',snowfree=True):\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_2020_{landcover_type}.pth\")\n",
    "    mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "    data_m[mask_t>=1]=np.nan\n",
    "    data_m=month_fill(data_m,mask_t>=1)\n",
    "    if snowfree:\n",
    "        data_m[:,:,:,-1,:]=0\n",
    "        data_m[:,:,:,:,-1]=0\n",
    "    z=albedo_all[:,0].transpose((0,2,3,1)).reshape((12,180,360,lc_len+1,1))*data_m\n",
    "    z[np.isnan(z)]=0\n",
    "    z=z*(mt.reshape(12,1,1,1,1)/365/24/3600)\n",
    "    if mode=='all':\n",
    "        return [np.nansum(z[:,:])/np.nansum(np.nanmean(data_m[:,:],axis=0))]\n",
    "    else:\n",
    "        return [np.nansum(z[:,s:e])/np.nansum(np.nanmean(data_m[:,s:e],axis=0))]\n",
    "def get_curve(s,t,snowfree,ismask=False):\n",
    "    season=[]\n",
    "    kk=get_01_season(s,t,landcover_type,snowfree=snowfree)\n",
    "    for y in range(2002,2021):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=1]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=1)\n",
    "        if snowfree:\n",
    "            data_m[:,:,:,-1,:]=0\n",
    "            data_m[:,:,:,:,-1]=0\n",
    "        z=albedo_all[:,y-2001].transpose((0,2,3,1)).reshape((12,180,360,1,lc_len+1))*data_m\n",
    "        n=0\n",
    "        for month in [[3,4,5],[6,7,8],[9,10,11],[12,1,2]]:\n",
    "            tt=[z[i-1,s:t] for i in month]\n",
    "            mts=np.array([mt[i-1] for i in month]).reshape(3,1,1,1,1)\n",
    "            tt=np.stack(tt)*mts/np.sum(mts)\n",
    "            mm_area=np.array([np.nansum(data_m[i-1,s:t]) for i in month]).mean()\n",
    "            kk[n].append(np.nansum(tt)/mm_area)\n",
    "            n+=1\n",
    "    for i in range(4):\n",
    "        kk2=[0]+[kk[i][j]-kk[i][0] for j in range(1,20)]\n",
    "        season.append(kk2)\n",
    "    stds=[np.std([season[0][i],season[1][i],season[2][i],season[3][i]]) for i in range(20)]\n",
    "    maxs=[np.max([season[0][i],season[1][i],season[2][i],season[3][i]]) for i in range(20)]\n",
    "    mins=[np.min([season[0][i],season[1][i],season[2][i],season[3][i]]) for i in range(20)]\n",
    "    kk2=get_01_all(s,t,landcover_type,snowfree=snowfree)\n",
    "    for y in tqdm(range(2002,2021)):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=1]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=1)\n",
    "        if snowfree:\n",
    "            data_m[:,:,:,-1,:]=0\n",
    "            data_m[:,:,:,:,-1]=0\n",
    "        z=albedo_all[:,y-2001].transpose((0,2,3,1)).reshape((12,180,360,1,lc_len+1))*data_m\n",
    "        z[np.isnan(z)]=0\n",
    "        z=z*(mt.reshape(12,1,1,1,1)/365/24/3600)\n",
    "        kk2.append(np.nansum(z[:,s:t])/np.nansum(np.nanmean(data_m[:,s:t],axis=0)))\n",
    "    kk=[0]+[kk2[j]-kk2[0] for j in range(1,len(kk2))]\n",
    "    return kk2,stds,maxs,mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the albedo change curve and seasonal variance.\n",
    "mt=[]\n",
    "for month in range(1,13):\n",
    "    now=datetime.datetime.strptime(f'2001-{month}-1', '%Y-%m-%d')\n",
    "    month_time=(now+relativedelta(months=1)-now).total_seconds()\n",
    "    mt.append(month_time)\n",
    "mt=np.array(mt)\n",
    "data=torch.load('../data/curve.pth')\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_{landcover_type}.pth')\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    albedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            albedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "            mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "\n",
    "    albedo_all[albedo_all==0]=np.nan\n",
    "    albedo_all[mask>=1]=np.nan\n",
    "    area_all=month_fill(area_all,mask>=1)\n",
    "    albedo_all=month_fill(albedo_all,mask>=1)  \n",
    "\n",
    "\n",
    "    kk2,stds,maxs,mins=get_curve(0,180,False)\n",
    "    data[f'G_snow_{landcover_type}']=kk2\n",
    "    data[f'G_snow_{landcover_type}_std']=stds\n",
    "    data[f'G_snow_{landcover_type}_max']=maxs\n",
    "    data[f'G_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(0,180,True)\n",
    "    data[f'G_snowfree_{landcover_type}']=kk2\n",
    "    data[f'G_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'G_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'G_snowfree_{landcover_type}_min']=mins\n",
    "\n",
    "    kk2,stds,maxs,mins=get_curve(0,24,False)\n",
    "    data[f'N_snow_{landcover_type}']=kk2\n",
    "    data[f'N_snow_{landcover_type}_std']=stds\n",
    "    data[f'N_snow_{landcover_type}_max']=maxs\n",
    "    data[f'N_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(-24,180,False)\n",
    "    data[f'S_snow_{landcover_type}']=kk2\n",
    "    data[f'S_snow_{landcover_type}_std']=stds\n",
    "    data[f'S_snow_{landcover_type}_max']=maxs\n",
    "    data[f'S_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(24,90-23,False)\n",
    "    data[f'TN_snow_{landcover_type}']=kk2\n",
    "    data[f'TN_snow_{landcover_type}_std']=stds\n",
    "    data[f'TN_snow_{landcover_type}_max']=maxs\n",
    "    data[f'TN_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(24,90-23,True)\n",
    "    data[f'TN_snowfree_{landcover_type}']=kk2\n",
    "    data[f'TN_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'TN_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'TN_snowfree_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90-23,90+24,False)\n",
    "    data[f'T_snow_{landcover_type}']=kk2\n",
    "    data[f'T_snow_{landcover_type}_std']=stds\n",
    "    data[f'T_snow_{landcover_type}_max']=maxs\n",
    "    data[f'T_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90-23,90+24,True)\n",
    "    data[f'T_snowfree_{landcover_type}']=kk2\n",
    "    data[f'T_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'T_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'T_snowfree_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90+24,-24,False)\n",
    "    data[f'TS_snow_{landcover_type}']=kk2\n",
    "    data[f'TS_snow_{landcover_type}_std']=stds\n",
    "    data[f'TS_snow_{landcover_type}_max']=maxs\n",
    "    data[f'TS_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90+24,-24,True)\n",
    "    data[f'TS_snowfree_{landcover_type}']=kk2\n",
    "    data[f'TS_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'TS_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'TS_snowfree_{landcover_type}_min']=mins\n",
    "\n",
    "\n",
    "torch.save(data,'../data/curve.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate albedo-multiply-area-multiply-time\n",
    "def get_aat(landcover_type):\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_{landcover_type}.pth')\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    bluealbedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            bluealbedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "            mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "\n",
    "    bluealbedo_all[np.isnan(bluealbedo_all)]=0\n",
    "    bluealbedo_all[mask>=2]=0\n",
    "    area_all[np.isnan(area_all)]=0\n",
    "    area_all[mask>=2]=0\n",
    "    kka=[]\n",
    "    kka.append(np.zeros((12,180, 360, lc_len+1, lc_len+1)))\n",
    "    for y in tqdm(range(2002,2021)):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=2]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=2)\n",
    "        data_y=np.nanmean(data_m,axis=0)\n",
    "        transf=np.nansum(data_y,axis=0)\n",
    "        transf=np.nansum(transf,axis=0)\n",
    "        dif_al=[]\n",
    "        for m in range(12):\n",
    "            exp1_2020=np.expand_dims(bluealbedo_all[m,y-2001],0).repeat(lc_len+1,axis=0)\n",
    "            exp0_2001=np.expand_dims(bluealbedo_all[m,0],1).repeat(lc_len+1,axis=1)\n",
    "            dif_al_m=(exp1_2020-exp0_2001).transpose((2,3,0,1))*data_m[m]\n",
    "            dif_al.append(dif_al_m)\n",
    "        dif_al=np.stack(dif_al)\n",
    "        kka.append(dif_al)\n",
    "    kka=np.stack(kka)\n",
    "    kka=kka.transpose((0,4,5,1,2,3))\n",
    "    kka2=kka*(mt.reshape((1,1,1,12,1,1)))\n",
    "    np.save(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy',kka2)\n",
    "    \n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    get_aat(landcover_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RF Matrix\n",
    "result2={}\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kka=np.load(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy')\n",
    "    kka=kka[-1]\n",
    "    kernels=torch.load(f\"/data2/hzy/albedo2/kernel/kernels.pth\")\n",
    "    rr=[]\n",
    "    for k in tqdm(['HadGEM2', 'HadGEM3', 'CAM5', 'ECHAM6', 'ERAI', 'ERA5']):\n",
    "        kka2=kka*kernels[k]\n",
    "        rr.append(np.nansum(kka2)/365/24/3600/5.1e8)\n",
    "        result2[f'{landcover_type}_{k}']=np.nansum(kka2.reshape((lc_len+1,lc_len+1,-1)),axis=-1)/365/24/3600/5.1e8\n",
    "torch.save(result2,f'../data/RF_ConfusionMatrix_2020.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the RF curve for six different kernels.\n",
    "for is_snowfree in ['snowdynamic','snowfree','snow']:\n",
    "    kernels=torch.load(f\"/data2/hzy/albedo2/kernel/kernels.pth\")\n",
    "    result={}\n",
    "    for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "        kka=np.load(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy')\n",
    "        if is_snowfree=='snowdynamic':\n",
    "            kka[:,:-1,:-1]=0\n",
    "        elif is_snowfree=='snowfree':\n",
    "            kka[:,-1,:]=0\n",
    "            kka[:,:,-1]=0\n",
    "        elif is_snowfree=='snow':\n",
    "            pass\n",
    "        else:\n",
    "            raise()\n",
    "        for key in ['HadGEM2', 'HadGEM3', 'CAM5', 'ECHAM6', 'ERAI', 'ERA5']:\n",
    "            print(f'{landcover_type}_{key}')\n",
    "            kka2=kka*kernels[key]*1e6\n",
    "            kka2[np.isnan(kka)]=0\n",
    "            kk2=[]\n",
    "            for y in range(2001,2021):\n",
    "                ay=np.nansum(kka2[y-2001,:,:])\n",
    "                kk2.append(ay)\n",
    "            kk=[0]+[kk2[j]-kk2[0] for j in range(1,len(kk2))]\n",
    "            result[f'{landcover_type}_{key}']=kk\n",
    "            # print('integrated RF',np.array(kk).sum())\n",
    "            del kka2\n",
    "            gc.collect()\n",
    "            torch.save(result,f'curve_energy_{is_snowfree}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the albedo change map induce different factor change.\n",
    "def filter_mode(data,mode,lc_len):\n",
    "    if mode=='snowfree':\n",
    "        data[:,:,:,-1,:]=0\n",
    "        data[:,:,:,:,-1]=0\n",
    "    elif mode=='snowfree-change':\n",
    "        data[:,:,:,-1,:]=0\n",
    "        data[:,:,:,:,-1]=0\n",
    "    elif mode=='all':\n",
    "        pass\n",
    "    elif mode=='snow':\n",
    "        data[:,:,:,:-1,:-1]=0\n",
    "    elif mode=='conversion':\n",
    "        data[:,:,:,-1,:]=0\n",
    "        data[:,:,:,:,-1]=0\n",
    "        for i in range(lc_len+1):\n",
    "            data[:,:,:,i,i]=0       \n",
    "    elif mode=='non_conversion':\n",
    "        mask_filter=np.zeros((lc_len+1,lc_len+1))\n",
    "        for i in range(lc_len):\n",
    "            mask_filter[i,i]=1\n",
    "        data=data*(mask_filter.reshape((1,1,1,lc_len+1,lc_len+1)))\n",
    "    else:\n",
    "        raise('ERROR MODE')\n",
    "    return data\n",
    "def get_albedo_change(landcover_type,mode,ismask):\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kk=[0]\n",
    "    kk_map=[]\n",
    "    kk_map.append(np.zeros((180,360)))\n",
    "    for y in range(2002,2021):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=1]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=1)\n",
    "        data_m2=data_m.copy()\n",
    "        data_m=filter_mode(data_m,mode,lc_len)\n",
    "        dif_al=[]\n",
    "        calibration=(y-2001)/10*0.007\n",
    "        for m in range(12):\n",
    "            exp1_2020=np.expand_dims(albedo_all[m,y-2001],0).repeat(lc_len+1,axis=0)\n",
    "            exp0_2001=np.expand_dims(albedo_all[m,0],1).repeat(lc_len+1,axis=1)\n",
    "            dif_al_m=(exp1_2020-exp0_2001).transpose((2,3,0,1))*data_m[m]\n",
    "            if ismask:\n",
    "                mask_dif=np.abs((exp1_2020-exp0_2001)/exp0_2001).transpose((2,3,0,1))\n",
    "                dif_al_m[mask_dif<calibration]=0\n",
    "            dif_al.append(dif_al_m*mt[m]/365/24/3600)\n",
    "        dif_al=np.stack(dif_al)\n",
    "        dif_al=np.nansum(dif_al,axis=-1)\n",
    "        dif_al=np.nansum(dif_al,axis=-1)\n",
    "        dif_al=np.nansum(dif_al,axis=0)\n",
    "        if not mode=='snowfree-change':\n",
    "            data_m=data_m2\n",
    "            area_tf=np.nansum(data_m,axis=-1)\n",
    "            area_tf=np.nansum(area_tf,axis=-1)\n",
    "            area_tf=np.nanmean(area_tf,axis=0)\n",
    "            kk.append(kk[0]+np.nansum(dif_al)/np.nansum(area_tf))\n",
    "        else:\n",
    "            area_tf=np.nansum(data_m,axis=-1)\n",
    "            area_tf=np.nansum(area_tf,axis=-1)\n",
    "            area_tf=np.nanmean(area_tf,axis=0)\n",
    "            area_tf2=np.nansum(data_m2,axis=-1)\n",
    "            area_tf2=np.nansum(area_tf2,axis=-1)\n",
    "            area_tf2=np.nanmean(area_tf2,axis=0)\n",
    "            snowfree_fraction=area_tf/area_tf2\n",
    "            kk.append(kk[0]+np.nansum(dif_al)/np.nansum(area_tf))\n",
    "            dif_al[snowfree_fraction<0.05]=0\n",
    "        kk_map.append(dif_al/area_tf)\n",
    "    kk_map=np.stack(kk_map)\n",
    "    print(kk)\n",
    "    plt.imshow(kk_map[-1],vmax=0.05,vmin=-0.05,cmap='seismic')\n",
    "    plt.show()\n",
    "    return kk,kk_map\n",
    "    \n",
    "\n",
    "result=torch.load('../data/albedo_change.pth')\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_{landcover_type}.pth')\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    albedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            albedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "            mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "    albedo_all[albedo_all==0]=np.nan\n",
    "    albedo_all[mask>=1]=np.nan\n",
    "    area_all=month_fill(area_all,mask>=1)\n",
    "    albedo_all=month_fill(albedo_all,mask>=1)\n",
    "    for mode in ['snowfree-change','snowfree','all','snow','conversion','non_conversion']:\n",
    "        print(mode)\n",
    "        result[f'{landcover_type}_{mode}_s']=get_albedo_change(landcover_type,mode,False)\n",
    "    result[f'{landcover_type}_all_c']=get_albedo_change(landcover_type,'all',True)\n",
    "    result[f'{landcover_type}_snowfree-change_c']=get_albedo_change(landcover_type,'snowfree-change',True)\n",
    "torch.save(result,'../data/albedo_change.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the RF map induce different factor change.\n",
    "def get_RF_map(mode,landcover_type):\n",
    "    print(landcover_type,mode)\n",
    "    def filter_mode(data,mode,lc_len):\n",
    "        if mode=='snowfree':\n",
    "            data[:,-1,:]=0\n",
    "            data[:,:,-1]=0\n",
    "        elif mode=='all':\n",
    "            pass\n",
    "        elif mode=='snow':\n",
    "            data[:,:-1,:-1]=0\n",
    "        elif mode=='conversion':\n",
    "            data[:,-1,:]=0\n",
    "            data[:,:,-1]=0\n",
    "            for i in range(lc_len+1):\n",
    "                data[:,i,i]=0       \n",
    "        elif mode=='non_conversion':\n",
    "            mask_filter=np.zeros((1,lc_len+1,lc_len+1))\n",
    "            for i in range(lc_len):\n",
    "                mask_filter[:,i,i]=1\n",
    "            data=data*(mask_filter.reshape((1,lc_len+1,lc_len+1,1,1)))\n",
    "        else:\n",
    "            raise('ERROR MODE')\n",
    "        return data\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kka2=filter_mode(kka.copy(),mode,lc_len)\n",
    "    kka2=np.sum(kka2,axis=1)\n",
    "    output=np.sum(kka2,axis=1)/365/24/3600/1e6/area_all\n",
    "    return output\n",
    "\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3']:\n",
    "    result={}\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    mt=[]\n",
    "    for month in range(1,13):\n",
    "        now=datetime.datetime.strptime(f'2001-{month}-1', '%Y-%m-%d')\n",
    "        month_time=(now+relativedelta(months=1)-now).total_seconds()\n",
    "        mt.append(month_time)\n",
    "    mt=np.array(mt)\n",
    "    kka=np.load(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy')\n",
    "    kernels=torch.load(f\"/data2/hzy/albedo2/kernel/kernels_6.pth\")\n",
    "    kka=kka*kernels['mean']*1e6\n",
    "    kka[np.isnan(kka)]=0\n",
    "    kka=np.sum(kka,axis=3)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_{landcover_type}.pth')\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "    area_all=month_fill(area_all,mask>=1)\n",
    "    area_all=np.nanmean(area_all,axis=0)\n",
    "    area_all=area_all\n",
    "    area_all=np.nansum(area_all,axis=1)\n",
    "\n",
    "    if landcover_type=='LCCS1':\n",
    "        result['snow']=get_RF_map('snow',landcover_type)\n",
    "        plt.imshow(result['snow'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "        plt.show()\n",
    "    result['snowfree']=get_RF_map('snowfree',landcover_type)\n",
    "    plt.imshow(result['snowfree'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "        \n",
    "    result['all']=get_RF_map('all',landcover_type)\n",
    "    plt.imshow(result['all'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "    \n",
    "    result['conversion']=get_RF_map('conversion',landcover_type)\n",
    "    plt.imshow(result['conversion'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "    \n",
    "    result['non_conversion']=get_RF_map('non_conversion',landcover_type)\n",
    "    plt.imshow(result['non_conversion'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "    torch.save(result,f'../data/map_RF_{landcover_type}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the tif files of albedo change map.\n",
    "all=[]\n",
    "all_c=[]\n",
    "snowfree=[]\n",
    "snowfree_change=[]\n",
    "snowfree_change_c=[]\n",
    "snow=[]\n",
    "result=torch.load('../data/albedo_change.pth')\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    for key in ['snow','non_conversion','conversion','all','snowfree','snowfree-change']:\n",
    "        if key=='snow':\n",
    "            snow.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "        elif key in ['non_conversion','conversion']:\n",
    "            kk=result[f'{landcover_type}_{key}_s'][1][-1]\n",
    "            kk[np.isnan(kk)]=0\n",
    "            tif_save(kk,f'../data/albedo_{key}_{landcover_type}_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "        elif key=='all':\n",
    "            all.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "            all_c.append(result[f'{landcover_type}_{key}_c'][1][-1])\n",
    "        elif key=='snowfree':\n",
    "            snowfree.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "        elif key=='snowfree-change':\n",
    "            snowfree_change.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "            snowfree_change_c.append(result[f'{landcover_type}_{key}_c'][1][-1])\n",
    "all=np.nanmean(np.stack(all),axis=0)\n",
    "all[np.isnan(all)]=0\n",
    "tif_save(all,f'../data/albedo_all_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "all_c=np.nanmean(np.stack(all_c),axis=0)\n",
    "all_c[np.isnan(all_c)]=0\n",
    "tif_save(all_c,f'../data/albedo_all_1degree_calibration.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snowfree=np.mean(np.stack(snowfree),axis=0).copy()\n",
    "snowfree[np.isnan(snowfree)]=0\n",
    "tif_save(snowfree,f'../data/albedo_snowfree_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snowfree_change=np.mean(np.stack(snowfree_change),axis=0).copy()\n",
    "snowfree_change[np.isnan(snowfree_change)]=0\n",
    "tif_save(snowfree_change,f'../data/albedo_snowfree-change_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snowfree_change_c=np.mean(np.stack(snowfree_change_c),axis=0).copy()\n",
    "snowfree_change_c[np.isnan(snowfree_change_c)]=0\n",
    "tif_save(snowfree_change_c,f'../data/albedo_snowfree-change_1degree_calibration.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snow=np.mean(np.stack(snow),axis=0).copy()\n",
    "snow[np.isnan(snow)]=0\n",
    "tif_save(snow,f'../data/albedo_snow_1degree_calibration.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the tif files of RF map.\n",
    "snow=[]\n",
    "all=[]\n",
    "snowfree=[]\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    result=torch.load(f'../data/map_RF_{landcover_type}.pth')\n",
    "    for key in result.keys():\n",
    "        if key=='snow':\n",
    "            snow.append(result[key][-1]-result[key][0])\n",
    "        elif key in ['non_conversion','conversion']:\n",
    "            kk=result[key][-1]-result[key][0]\n",
    "            kk[np.isnan(kk)]=0\n",
    "            tif_save(kk,f'../data/RF_{key}-{landcover_type}_1degree.tif',(-180,1,0,90,0,-1),p='4326')\n",
    "        elif key=='all':\n",
    "            all.append(result[key][-1]-result[key][0])\n",
    "        elif key=='snowfree':\n",
    "            snowfree.append(result[key][-1]-result[key][0])\n",
    "all=np.nanmean(np.stack(all),axis=0)\n",
    "all[np.isnan(all)]=0\n",
    "tif_save(all,f'../data/RF_all_1degree.tif',(-180,1,0,90,0,-1),p='4326')\n",
    "\n",
    "snowfree=np.nanmean(np.stack(snowfree),axis=0)\n",
    "snowfree[np.isnan(snowfree)]=0\n",
    "tif_save(snowfree,f'../data/RF_snowfree_1degree.tif',(-180,1,0,90,0,-1),p='4326')\n",
    "\n",
    "snow=np.nanmean(np.stack(snow),axis=0)\n",
    "snow[np.isnan(snow)]=0\n",
    "tif_save(snowfree,f'../data/RF_snow_1degree.tif',(-180,1,0,90,0,-1),p='4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export contribution map of PV, NPV and SWC basde on LCCS2\n",
    "landcover_type='LCCS2'\n",
    "result={}\n",
    "lc_len=10\n",
    "k=[]\n",
    "for i in range(40076):\n",
    "    k.append((pi/180.0)*R*R*abs(math.sin((90.00220831593487-pixel_with*i)/180.0*pi) - math.sin((90.00220831593487-pixel_with*(i+1))/180.0*pi)) * pixel_with)\n",
    "lc01=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2001_{landcover_type}_v2.tif').read(1)\n",
    "lc20=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2020_{landcover_type}_v2.tif').read(1)\n",
    "lc01[lc01==100]=20\n",
    "lc01[lc01==0]=20\n",
    "lc20[lc20==100]=20\n",
    "lc20[lc20==0]=20\n",
    "\n",
    "z=np.zeros(lc01.shape)\n",
    "z[lc01==lc20]=1\n",
    "for i in [1,2,20]:\n",
    "    z[lc01==i]=0\n",
    "lc01[lc01==20]=0\n",
    "lc01[lc20==20]=0\n",
    "lc01[lc01!=0]=1\n",
    "landarea=np.array(k).reshape(-1,1)*lc01\n",
    "\n",
    "for b in ['NDVI','SSI','NMDI']:\n",
    "    for key in ['mean']:\n",
    "        print(b)\n",
    "        img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}_{key}_eg_final.tif').read(1)\n",
    "        img[np.isnan(img)]=0\n",
    "        img=z*img\n",
    "        tif_save(img,f'/data2/hzy/ssd_hzy/G3/{b}_{key}_eg_final_map.tif',global_trf,p='4326')\n",
    "for b in ['NDVI','SSI','NMDI']:\n",
    "    print(b)\n",
    "    img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}_final.tif').read(1)\n",
    "    img[np.isnan(img)]=0\n",
    "    img=z*img\n",
    "    tif_save(img,f'/data2/hzy/ssd_hzy/G3/{b}_final_map.tif',global_trf,p='4326')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly contribution of PV, NPV and SWC to GLMA\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    k=[]\n",
    "    for i in range(40076):\n",
    "        k.append((pi/180.0)*R*R*abs(math.sin((90.00220831593487-pixel_with*i)/180.0*pi) - math.sin((90.00220831593487-pixel_with*(i+1))/180.0*pi)) * pixel_with)\n",
    "    lc01=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2001_{landcover_type}_v2.tif').read(1)\n",
    "    lc20=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2020_{landcover_type}_v2.tif').read(1)\n",
    "    lc01[lc01==100]=20\n",
    "    lc01[lc01==0]=20\n",
    "    lc20[lc20==100]=20\n",
    "    lc20[lc20==0]=20\n",
    "    z=np.zeros(lc01.shape)\n",
    "    z[lc01==lc20]=1\n",
    "    if landcover_type=='IGBP':\n",
    "        for i in [2,16,20]:\n",
    "            z[lc01==i]=0\n",
    "    else:\n",
    "        for i in [1,2,20]:\n",
    "            z[lc01==i]=0\n",
    "    lc01[lc01==20]=0\n",
    "    lc01[lc20==20]=0\n",
    "    lc01[lc01!=0]=1\n",
    "    landarea=np.array(k).reshape(-1,1)*lc01\n",
    "    del lc01,lc20,k\n",
    "    gc.collect()\n",
    "    print(landarea.sum())\n",
    "    ddd={}\n",
    "    for b in ['NDVI','SSI','NMDI']:\n",
    "        print(b)\n",
    "        ddd[b]=[]\n",
    "        for m in tqdm(range(1,13)):\n",
    "            img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}{m}_albedo_final.tif').read(1)\n",
    "            img[np.isnan(img)]=0\n",
    "            img=img*z\n",
    "            img=landarea*img\n",
    "            ddd[b].append(img.sum()/landarea.sum())\n",
    "            del img\n",
    "            gc.collect()\n",
    "    torch.save(ddd,f'seasons_model_dict_{landcover_type}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contribution of PV, NPV and SWC to GLMA over different LULC types\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    result={}\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    k=[]\n",
    "    for i in range(40076):\n",
    "        k.append((pi/180.0)*R*R*abs(math.sin((90.00220831593487-pixel_with*i)/180.0*pi) - math.sin((90.00220831593487-pixel_with*(i+1))/180.0*pi)) * pixel_with)\n",
    "        \n",
    "    lc01=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2001_{landcover_type}_v2.tif').read(1)\n",
    "    lc20=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2020_{landcover_type}_v2.tif').read(1)\n",
    "    lc01[lc01==100]=20\n",
    "    lc01[lc01==0]=20\n",
    "    lc20[lc20==100]=20\n",
    "    lc20[lc20==0]=20\n",
    "\n",
    "    z=np.zeros(lc01.shape)\n",
    "    z[lc01==lc20]=lc01[lc01==lc20]\n",
    "    if landcover_type=='IGBP':\n",
    "        for i in [2,16,20]:\n",
    "            z[lc01==i]=0\n",
    "    else:\n",
    "        for i in [1,2,20]:\n",
    "            z[lc01==i]=0\n",
    "    lc01[lc01==20]=0\n",
    "    lc01[lc20==20]=0\n",
    "    lc01[lc01!=0]=1\n",
    "\n",
    "\n",
    "\n",
    "    landarea=np.array(k).reshape(-1,1)*lc01\n",
    "    del lc01,lc20,k\n",
    "    gc.collect()\n",
    "    print(landarea.sum())\n",
    "\n",
    "    \n",
    "    for b in ['NDVI','SSI','NMDI']:\n",
    "        result_b=[]\n",
    "        print(b)\n",
    "        img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}_final.tif').read(1)\n",
    "        img[np.isnan(img)]=0\n",
    "        img[z==0]=0\n",
    "        img=landarea*img\n",
    "        for i in range(1,lc_len+1):\n",
    "            img2=img.copy()\n",
    "            img2[z!=i]=0\n",
    "            zz=img2.sum()/landarea.sum()\n",
    "            result_b.append(zz)\n",
    "            print(type_code[landcover_type][i-1],zz)\n",
    "            \n",
    "            del img2\n",
    "            gc.collect()\n",
    "        result[b]=result_b\n",
    "        del img\n",
    "        gc.collect()\n",
    "    \n",
    "    del z\n",
    "    gc.collect()\n",
    "    torch.save(result,f'type_model_dict_{landcover_type}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculte trend and significance test \n",
    "def get_ALLUMs_mean():\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    albedo_all_four=[]\n",
    "    area_all_four=[]\n",
    "    for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "        print(landcover_type)\n",
    "        if landcover_type=='LCCS2':\n",
    "            lc_len=10\n",
    "        elif landcover_type=='LCCS3':\n",
    "            lc_len=9\n",
    "        elif landcover_type=='IGBP':\n",
    "            lc_len=16\n",
    "        elif landcover_type=='LCCS1':\n",
    "            lc_len=15 \n",
    "        \n",
    "        stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_{landcover_type}.pth')\n",
    "        albedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "        area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "        mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "\n",
    "        for y in range(2001,2021):\n",
    "            for m in range(1,13):\n",
    "                albedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "                area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "                mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "        albedo_all[albedo_all==0]=np.nan\n",
    "        albedo_all[mask>=1]=np.nan\n",
    "        area_all=month_fill(area_all,mask>=1)\n",
    "        albedo_all=month_fill(albedo_all,mask>=1)\n",
    "        zz=area_all*albedo_all\n",
    "        area_snowfree=np.nansum(area_all[:,:,:-1],axis=2)\n",
    "        area_snow=area_all[:,:,-1]\n",
    "        albedo_snowfree=np.nansum(zz[:,:,:-1],axis=2)/area_snowfree\n",
    "        albedo_snow=zz[:,:,-1]/area_snow\n",
    "        albedo_two=np.stack([albedo_snowfree,albedo_snow]).transpose([1,2,0,3,4])\n",
    "        area_two=np.stack([area_snowfree,area_snow]).transpose([1,2,0,3,4])\n",
    "        albedo_all_four.append(albedo_two)\n",
    "        area_all_four.append(area_two)\n",
    "    albedo_all=np.nanmean(np.stack(albedo_all_four),axis=0)\n",
    "    area_all=np.nanmean(np.stack(area_all_four),axis=0)\n",
    "    return albedo_all,area_all\n",
    "\n",
    "def trend_map(a):\n",
    "    def LinearRegression_trend(kk):\n",
    "        if np.isnan(kk).any():\n",
    "            mask=0\n",
    "            return [-1,0,mask]\n",
    "        else:\n",
    "            mask=1\n",
    "            result = mk(kk)\n",
    "            return [result[-1],result[0],mask]\n",
    "    k=[LinearRegression_trend(a[i]) for i in range(a.shape[0])]\n",
    "    ps=[k[i][0] for i in range(len(k))]\n",
    "    slopes=[k[i][1] for i in range(len(k))]\n",
    "    masks=[k[i][2] for i in range(len(k))]\n",
    "    return [ps,slopes,masks]\n",
    "\n",
    "def get_land_perc():\n",
    "    land_area=np.nanmean(area_all,axis=0)\n",
    "    land_area=np.nanmean(land_area,axis=0)\n",
    "    land_area=np.nansum(land_area,axis=0)\n",
    "    pi = 3.1415926\n",
    "    R = 6371007.181\t\n",
    "    pixel_with=1\n",
    "    lats=[90-i*1 for i in range(180)]\n",
    "    pixels_area=[(pi/180.0)*R*R*abs(math.sin((lat)/180.0*pi) - math.sin((lat-pixel_with)/180.0*pi)) * pixel_with for lat in lats]\n",
    "    area_map=np.array([[1]*360])*(np.array([pixels_area]).T)/1000000\n",
    "    land_area=land_area.reshape((60,-1,360)).reshape((60,3,120,-1)).transpose((0,2,1,3))\n",
    "    land_area=np.sum(land_area,axis=-1)\n",
    "    land_area=np.sum(land_area,axis=-1)\n",
    "    area_map=area_map.reshape((60,-1,360)).reshape((60,3,120,-1)).transpose((0,2,1,3))\n",
    "    area_map=np.sum(area_map,axis=-1)\n",
    "    area_map=np.sum(area_map,axis=-1)\n",
    "    land_perc=land_area/area_map\n",
    "    return land_perc\n",
    "\n",
    "def get_p(zz,nan=True):\n",
    "    zz2=zz.reshape((20,60,-1,360)).reshape((20,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((20,60,120,-1))\n",
    "    if nan:\n",
    "        zz2=np.nanmean(zz2,axis=-1).reshape(20,-1).transpose((1,0))\n",
    "    else:\n",
    "        zz2=np.mean(zz2,axis=-1).reshape(20,-1).transpose((1,0))\n",
    "    rr=trend_map(zz2)\n",
    "    slope_3=np.array(rr[1]).reshape(60,120)\n",
    "    p=np.array(rr[0]).reshape(60,120)\n",
    "    p[p==-1]=1\n",
    "    p[p<0.05]=0\n",
    "    p[p>=0.05]=1\n",
    "    p[p==0]=2\n",
    "    p-=1\n",
    "    return p,slope_3\n",
    "\n",
    "def get_calibration_mask(input_albedo,slope,mask=None):\n",
    "    calibration_limit=input_albedo[0]*0.0007\n",
    "    calibration_mask=np.zeros((180,360))\n",
    "    calibration_mask[slope>calibration_limit]=1\n",
    "    calibration_mask[slope<(-calibration_limit)]=1\n",
    "    if mask is not None:\n",
    "        calibration_mask[mask==0]=1\n",
    "    calibration_mask[slope==0]=1\n",
    "    calibration_limit_nan=np.zeros((180, 360))\n",
    "    calibration_limit_nan[slope>calibration_limit]=1\n",
    "    calibration_limit_nan[slope<(-calibration_limit)]=1\n",
    "    calibration_limit_nan[calibration_limit_nan==0]=np.nan\n",
    "    return calibration_mask,calibration_limit_nan\n",
    "def to_gpd(p):\n",
    "    lat=np.array([88.5-3*i for i in range(60)]).reshape((60,1))*np.ones((1,120))\n",
    "    lon=np.array([-178.5+3*i for i in range(120)]).reshape((1,120))*np.ones((60,1))\n",
    "    lat[p==0]=0\n",
    "    lon[p==0]=0\n",
    "    lat=list(lat.reshape((-1,1)))\n",
    "    lon=list(lon.reshape((-1,1)))\n",
    "    coords=[[lat[i],lon[i]] for i in range(len(lat)) if lat[i]!=0]\n",
    "    from shapely import geometry\n",
    "    coords_geo=[geometry.Point(i[1],i[0]) for i in coords]\n",
    "    gpdd=gpd.GeoDataFrame.from_dict({'id':list(range(len(coords_geo))),'geometry':coords_geo})\n",
    "    gpdd=gpdd.set_crs(4326)\n",
    "    return gpdd\n",
    "def calculation_sig_perc(area,area_mask,p,slope):\n",
    "    # area_mask=area*calibration_limit_nan\n",
    "    area2=area.reshape((20,60,-1,360)).reshape((20,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((20,60,120,-1))\n",
    "    area2_mask=area_mask.reshape((20,60,-1,360)).reshape((20,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((20,60,120,-1))\n",
    "    area2=np.nansum(area2,axis=-1)\n",
    "    area2=np.nanmean(area2,axis=0)\n",
    "    area2_mask=np.nansum(area2_mask,axis=-1)\n",
    "    area2_mask=np.nanmean(area2_mask,axis=0)\n",
    "    area2_mask[np.isnan(area2_mask)]=0\n",
    "    area_sig=area2_mask.copy()\n",
    "    area_sig[p==0]=0\n",
    "    area_increase=area_sig.copy()\n",
    "    area_increase[slope<0]=0\n",
    "    area_decrease=area_sig.copy()\n",
    "    area_decrease[slope>0]=0\n",
    "    print(area_sig.sum()/area2.sum()*100,'%',area_increase.sum()/area2.sum()*100,'%',area_decrease.sum()/area2.sum()*100,'%')\n",
    "\n",
    "def t_test(a,b):\n",
    "    if len(a.shape)==1:\n",
    "        a=a.reshape((1,-1))\n",
    "    if len(b.shape)==1:\n",
    "        b=b.reshape((1,-1))\n",
    "    def t_test_single(a_single,b_single):\n",
    "        r=stats.ttest_ind(b_single, a_single)\n",
    "        return [(r.__getattribute__(\"statistic\")>0).astype(np.int8),(r.__getattribute__(\"pvalue\")<0.05).astype(np.int8)]\n",
    "    k=[t_test_single(a[i],b[i]) for i in range(a.shape[0])]\n",
    "    k2=[k[i][0] for i in range(len(k))]\n",
    "    k3=[k[i][1] for i in range(len(k))]\n",
    "    return [k2,k3]\n",
    "\n",
    "def get_p_twosided(zz,nan=True):\n",
    "    zz_a2=np.stack(zz[:10]).reshape((10,60,-1,360)).reshape((10,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((10,60,120,-1))\n",
    "    zz_b2=np.stack(zz[10:]).reshape((10,60,-1,360)).reshape((10,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((10,60,120,-1))\n",
    "    if nan:\n",
    "        zz_a2=np.nanmean(zz_a2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "        zz_b2=np.nanmean(zz_b2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "    else:\n",
    "        zz_a2=np.mean(zz_a2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "        zz_b2=np.mean(zz_b2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "    t2,p2=t_test(zz_a2,zz_b2)\n",
    "    p_array2=np.array(p2).reshape(60,120)\n",
    "    t_array2=np.array(t2).reshape(60,120)\n",
    "    t_array2[t_array2==0]=-1\n",
    "    return t_array2,p_array2\n",
    "\n",
    "def get_area_mask(mode):\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    area_mask=[]\n",
    "    for landcover_type in tqdm(['LCCS1','LCCS2','LCCS3','IGBP']):\n",
    "        if landcover_type=='LCCS2':\n",
    "            lc_len=10\n",
    "        elif landcover_type=='LCCS3':\n",
    "            lc_len=9\n",
    "        elif landcover_type=='IGBP':\n",
    "            lc_len=16\n",
    "        elif landcover_type=='LCCS1':\n",
    "            lc_len=15 \n",
    "        \n",
    "        stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_{landcover_type}.pth')\n",
    "        _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "        albedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "        area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "        mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "\n",
    "        for y in range(2001,2021):\n",
    "            for m in range(1,13):\n",
    "                albedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "                area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "                mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "        albedo_all[albedo_all==0]=np.nan\n",
    "        albedo_all[mask>=1]=np.nan\n",
    "        area_all=month_fill(area_all,mask>=1)\n",
    "        albedo_all=month_fill(albedo_all,mask>=1)\n",
    "        \n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=1]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=1)\n",
    "        data_m=filter_mode(data_m,mode,lc_len)\n",
    "        calibration=(y-2001)*0.0007\n",
    "        for m in range(12):\n",
    "            exp1_2020=np.expand_dims(albedo_all[m,y-2001],0).repeat(lc_len+1,axis=0)\n",
    "            exp0_2001=np.expand_dims(albedo_all[m,0],1).repeat(lc_len+1,axis=1)\n",
    "            mask_dif=np.abs((exp1_2020-exp0_2001)/exp0_2001).transpose((2,3,0,1))\n",
    "            data_m[m][mask_dif<calibration]=0\n",
    "        data_m=data_m*(mt.reshape(12,1,1,1,1)/365/24/3600)\n",
    "        area_tf=np.nansum(data_m,axis=-1)\n",
    "        area_tf=np.nansum(area_tf,axis=-1)\n",
    "        area_tf=np.nansum(area_tf,axis=0)\n",
    "        area_mask.append(area_tf)\n",
    "    area_mask2=np.nanmean(np.stack(area_mask),axis=0)\n",
    "    return area_mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trend for albedo and MK test\n",
    "mt=[]\n",
    "for month in range(1,13):\n",
    "    now=datetime.datetime.strptime(f'2001-{month}-1', '%Y-%m-%d')\n",
    "    month_time=(now+relativedelta(months=1)-now).total_seconds()\n",
    "    mt.append(month_time)\n",
    "mt=np.array(mt)\n",
    "albedo_all,area_all=get_ALLUMs_mean()\n",
    "area_all=area_all*(mt.reshape(12,1,1,1,1)/365/24/3600)\n",
    "land_perc=get_land_perc()\n",
    "torch.save(land_perc,'../data/land_perc.pth')\n",
    "for key in ['all','snowfree','snow-change5perc']:\n",
    "    print(key)\n",
    "    zz=albedo_all*area_all\n",
    "    if key=='snow-change5perc':\n",
    "        zz=np.nansum(zz[:,:,-1],axis=0)\n",
    "        # zz[zz==0]=np.nan\n",
    "        area=np.nansum(area_all[:,:,-1],axis=0)\n",
    "    elif key=='snowfree':\n",
    "        zz=np.nansum(np.nansum(zz[:,:,:-1],axis=0),axis=1)\n",
    "        area=np.nansum(np.nansum(area_all[:,:,:-1],axis=0),axis=1)\n",
    "    elif key=='all':\n",
    "        zz=np.nansum(np.nansum(zz,axis=0),axis=1)\n",
    "        area=np.nansum(np.nansum(area_all,axis=0),axis=1)\n",
    "    else:\n",
    "        raise()\n",
    "    \n",
    "    all_albedo=zz/area\n",
    "    zz=all_albedo.copy()\n",
    "\n",
    "    if not key=='all':\n",
    "        if key=='snow-change5perc':\n",
    "            mean_area=np.nansum(np.nansum(area_all,axis=0),axis=0)\n",
    "            snow_area_perc=mean_area[-1]/np.nansum(mean_area,axis=0)\n",
    "        else:\n",
    "            mean_area=np.nansum(np.nansum(area_all,axis=0),axis=0)\n",
    "            snow_area_perc=mean_area[0]/np.nansum(mean_area,axis=0)\n",
    "\n",
    "        mask=(snow_area_perc>0.05).astype(np.float32)\n",
    "        mask[mask==0]=np.nan\n",
    "        zz=zz*mask\n",
    "        area*=mask\n",
    "        area[np.isnan(area)]=0\n",
    "    else:\n",
    "        mask=None\n",
    "\n",
    "    rr=trend_map(zz.reshape((20,-1)).transpose((1,0)))\n",
    "    slope=np.array(rr[1]).reshape(180,360)\n",
    "    tif_save(slope,f'../data/albedo_{key}_trend_1degree.tif',(-180, 1.0, 0.0, 90, 0.0, -1.0),p='4326')\n",
    "    calibration_mask,calibration_limit_nan=get_calibration_mask(all_albedo,slope,mask)\n",
    "    tif_save(calibration_mask,f'../data/albedo_{key}_trend_1degree_mask.tif',(-180, 1.0, 0.0, 90, 0.0, -1.0),p='4326')\n",
    "\n",
    "    zz_calibration_limit=zz*calibration_limit_nan\n",
    "    p,slope_3=get_p(zz_calibration_limit,nan=True)\n",
    "    p2=p.copy()\n",
    "    p2[land_perc<0.05]=0\n",
    "    gpdd=to_gpd(p2)\n",
    "    gpdd.to_file(f'../data/significant_{key}_mannkendall_calibrationmask.shp')\n",
    "    calculation_sig_perc(area,area*calibration_limit_nan,p,slope_3)\n",
    "    p,slope_3=get_p(zz,nan=True)\n",
    "    p2=p.copy()\n",
    "    p2[land_perc<0.05]=0\n",
    "    gpdd=to_gpd(p2)\n",
    "    gpdd.to_file(f'../data/significant_{key}_mannkendall.shp')\n",
    "    calculation_sig_perc(area,area,p,slope_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trend for RF and MK test\n",
    "for key in ['all']:\n",
    "    print(key)\n",
    "    kk=[]\n",
    "    for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "        result=torch.load(f'../data/map_RF_{landcover_type}.pth')\n",
    "        kk.append(result['all'])\n",
    "    zz=np.nanmean(np.stack(kk),axis=0)\n",
    "    area=np.nansum(np.nansum(area_all,axis=0),axis=1)\n",
    "    mask=None\n",
    "    rr=trend_map(zz.reshape((20,-1)).transpose((1,0)))\n",
    "    slope=np.array(rr[1]).reshape(180,360)\n",
    "    tif_save(slope,f'../data/RF_{key}_trend_1degree.tif',(-180, 1.0, 0.0, 90, 0.0, -1.0),p='4326')\n",
    " \n",
    "    p,slope_3=get_p(zz,nan=True)\n",
    "    p2=p.copy()\n",
    "    p2[land_perc<0.05]=0\n",
    "    gpdd=to_gpd(p2)\n",
    "    gpdd.to_file(f'../data/RF_significant_{key}_mannkendall.shp')\n",
    "    calculation_sig_perc(area,area,p,slope_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two-sided t test for albedo\n",
    "area_mask_all=get_area_mask('all')\n",
    "area_mask_snowfree=get_area_mask('snowfree-change')\n",
    "result=torch.load('../data/albedo_change.pth')\n",
    "land_perc=torch.load('../data/land_perc.pth')\n",
    "for key in ['all','snowfree-change']:\n",
    "    if key=='all':\n",
    "        area=np.nansum(np.nansum(area_all[:,:,:],axis=0),axis=1)\n",
    "    elif key=='snowfree-change':\n",
    "        area=np.nansum(np.nansum(area_all[:,:,:-1],axis=0),axis=1)\n",
    "    else:\n",
    "        raise()\n",
    "    for mode in ['s','c']:\n",
    "        kk=[]\n",
    "        for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "            kk.append(result[f'{landcover_type}_{key}_{mode}'][1])\n",
    "        zz=np.nanmean(np.stack(kk),axis=0)\n",
    "        t,p=get_p_twosided(zz,nan=True)\n",
    "        p2=p.copy()\n",
    "        p2[land_perc<0.05]=0\n",
    "        gpdd=to_gpd(p2)\n",
    "        gpdd.to_file(f'../data/significant_{key}_{mode}.shp')\n",
    "        print(key,mode)\n",
    "        if mode=='c':\n",
    "            if key=='all':\n",
    "                calculation_sig_perc(area,np.expand_dims(area_mask_all, 0).repeat(20, axis=0),p,t)\n",
    "            else:\n",
    "                calculation_sig_perc(area,np.expand_dims(area_mask_snowfree, 0).repeat(20, axis=0),p,t)\n",
    "        else:\n",
    "            calculation_sig_perc(area,area,p,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two-sided t test for RF\n",
    "for key in ['all']:\n",
    "    area=np.nansum(np.nansum(area_all[:,:,:],axis=0),axis=1)\n",
    "    kk=[]\n",
    "    for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "        result=torch.load(f'../data/map_RF_{landcover_type}.pth')\n",
    "        kk.append(result['all'])\n",
    "    zz=np.nanmean(np.stack(kk),axis=0)\n",
    "    \n",
    "    t,p=get_p_twosided(zz,nan=True)\n",
    "    p2=p.copy()\n",
    "    p2[land_perc<0.05]=0\n",
    "    gpdd=to_gpd(p2)\n",
    "    gpdd.to_file(f'../data/RF_significant_{key}.shp')\n",
    "    print(key,mode)\n",
    "    calculation_sig_perc(area,area,p,t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonhzy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
