{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from tool import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:22<00:00,  1.90s/it]\n",
      "100%|██████████| 12/12 [00:33<00:00,  2.78s/it]\n",
      "100%|██████████| 12/12 [02:11<00:00, 10.95s/it]\n",
      "100%|██████████| 12/12 [00:41<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    for month in tqdm(range(1,13)):\n",
    "\n",
    "        original_dataset=nc.Dataset(f\"/data2/hzy/albedo2/albedo_information_nc2/fill/month_{month}_{landcover_type}.nc\",'r')\n",
    "        new_dataset = netCDF4.Dataset(f\"/data2/hzy/albedo2/albedo_information_nc2/publish/month_{month}_{landcover_type}.nc\", 'w')\n",
    "        for dim_name, dim in original_dataset.dimensions.items():\n",
    "            new_dataset.createDimension(dim_name, len(dim))\n",
    "        for var_name, var in original_dataset.variables.items():\n",
    "            \n",
    "            if not 'DWS' in var_name:\n",
    "                if not 'no_nan' in var_name:\n",
    "                    new_var = new_dataset.createVariable(var_name, var.dtype, var.dimensions)\n",
    "                    new_var[:] = var[:]\n",
    "        original_dataset.close()\n",
    "        new_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data\n",
    "def get_stat(sza_L_mode,band_mode,landcover_type):\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15  \n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    if band_mode=='shortwave':  \n",
    "        WB_sky_fraction=torch.load(\"/data/hk/albedo/white_sky_fraction/white_sky_fraction2.pth\")\n",
    "    else:\n",
    "        if band_mode not in ['nir','vis']:\n",
    "            raise('Error band_mode!')\n",
    "        WB_sky_fraction=torch.load(f\"/data/hk/albedo/white_sky_fraction/white_sky_fraction_{band_mode}2.pth\")\n",
    "    landtypes=['landtype'+str(i) for i in range(1,lc_len+1)]+['snow']\n",
    "    stat={}\n",
    "    for month in range(1,13):\n",
    "        NC=nc.Dataset(f\"/data2/hzy/albedo2/albedo_information_nc2/fill/month_{month}_{landcover_type}.nc\",'r')\n",
    "        for year in range(2001,2021):\n",
    "            types_stat_albedo=[]\n",
    "            types_stat_area=[]\n",
    "            for type in landtypes:\n",
    "                bs=NC.variables[f'albedo_BSA_{band_mode}-{type}'][year-2001]\n",
    "                ws=NC.variables[f'albedo_WSA_{band_mode}-{type}'][year-2001]\n",
    "                area_type=NC.variables[f'area-{type}'][year-2001] \n",
    "                area_land=NC.variables[f'area-land'][year-2001] \n",
    "                wf=WB_sky_fraction[(year-2001)*12+month-1]\n",
    "                if sza_L_mode=='70':\n",
    "                    area_type[sza_L[year-2001,month-1]>=1]=np.nan###\n",
    "                elif sza_L_mode=='85':\n",
    "                    area_type[sza_L[year-2001,month-1]>=2]=np.nan###\n",
    "                else:\n",
    "                    raise('Error sza_L_mode')\n",
    "                types_stat_albedo.append(((ws*wf+bs*(1-wf))/1000))      \n",
    "                types_stat_area.append(area_type)\n",
    "            types_stat_albedo=np.stack(types_stat_albedo)\n",
    "            types_stat_area=np.stack(types_stat_area)\n",
    "            types_stat_albedo[types_stat_albedo==0]=np.nan\n",
    "            stat[f'{year}-{month}-albedo']=types_stat_albedo\n",
    "            stat[f'{year}-{month}-typearea']=types_stat_area\n",
    "            stat[f'{year}-landarea']=area_land   \n",
    "        NC.close() \n",
    "    save_path=f'/data2/hzy/albedo2/data/stat_sw-mean_sza-{sza_L_mode}_band-{band_mode}_{landcover_type}.pth'\n",
    "    print(save_path)\n",
    "    torch.save(stat,save_path)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_LCCS1.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_LCCS1.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_LCCS2.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_LCCS2.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_LCCS3.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_LCCS3.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_IGBP.pth\n",
      "/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_IGBP.pth\n"
     ]
    }
   ],
   "source": [
    "# Organize data\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    for sza_L_mode in ['70','85']:\n",
    "        for band_mode in ['shortwave']:\n",
    "            get_stat(sza_L_mode,band_mode,landcover_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the albedo change curve and seasonal variance.\n",
    "def get_01_season(s,e,landcover_type,mode='region',snowfree=True):\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kk=[[],[],[],[]]\n",
    "    data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_2020_{landcover_type}.pth\")\n",
    "    mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "    data_m[mask_t>=1]=np.nan\n",
    "    data_m=month_fill(data_m,mask_t>=1)\n",
    "    if snowfree:\n",
    "        data_m[:,:,:,-1,:]=0\n",
    "        data_m[:,:,:,:,-1]=0\n",
    "    z=albedo_all[:,0].transpose((0,2,3,1)).reshape((12,180,360,lc_len+1,1))*data_m\n",
    "    n=0\n",
    "    for month in [[3,4,5],[6,7,8],[9,10,11],[12,1,2]]:\n",
    "        if mode=='all':\n",
    "            tt=[z[i-1,:] for i in month]\n",
    "        else:\n",
    "            tt=[z[i-1,s:e] for i in month]\n",
    "        mts=np.array([mt[i-1] for i in month]).reshape(3,1,1,1,1)\n",
    "        tt=np.stack(tt)*mts/np.sum(mts)\n",
    "        if mode=='all':\n",
    "            mm_area=np.array([np.nansum(data_m[i-1,:]) for i in month]).mean()\n",
    "        else:\n",
    "            mm_area=np.array([np.nansum(data_m[i-1,s:e]) for i in month]).mean()\n",
    "        kk[n].append(np.nansum(tt)/mm_area)\n",
    "        n+=1\n",
    "    return kk\n",
    "def get_01_all(s,e,landcover_type,mode='region',snowfree=True):\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_2020_{landcover_type}.pth\")\n",
    "    mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "    data_m[mask_t>=1]=np.nan\n",
    "    data_m=month_fill(data_m,mask_t>=1)\n",
    "    if snowfree:\n",
    "        data_m[:,:,:,-1,:]=0\n",
    "        data_m[:,:,:,:,-1]=0\n",
    "    z=albedo_all[:,0].transpose((0,2,3,1)).reshape((12,180,360,lc_len+1,1))*data_m\n",
    "    z[np.isnan(z)]=0\n",
    "    z=z*(mt.reshape(12,1,1,1,1)/365/24/3600)\n",
    "    if mode=='all':\n",
    "        return [np.nansum(z[:,:])/np.nansum(np.nanmean(data_m[:,:],axis=0))]\n",
    "    else:\n",
    "        return [np.nansum(z[:,s:e])/np.nansum(np.nanmean(data_m[:,s:e],axis=0))]\n",
    "def get_curve(s,t,snowfree,ismask=False):\n",
    "    season=[]\n",
    "    kk=get_01_season(s,t,landcover_type,snowfree=snowfree)\n",
    "    for y in range(2002,2021):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=1]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=1)\n",
    "        if snowfree:\n",
    "            data_m[:,:,:,-1,:]=0\n",
    "            data_m[:,:,:,:,-1]=0\n",
    "        z=albedo_all[:,y-2001].transpose((0,2,3,1)).reshape((12,180,360,1,lc_len+1))*data_m\n",
    "        n=0\n",
    "        for month in [[3,4,5],[6,7,8],[9,10,11],[12,1,2]]:\n",
    "            tt=[z[i-1,s:t] for i in month]\n",
    "            mts=np.array([mt[i-1] for i in month]).reshape(3,1,1,1,1)\n",
    "            tt=np.stack(tt)*mts/np.sum(mts)\n",
    "            mm_area=np.array([np.nansum(data_m[i-1,s:t]) for i in month]).mean()\n",
    "            kk[n].append(np.nansum(tt)/mm_area)\n",
    "            n+=1\n",
    "    for i in range(4):\n",
    "        kk2=[0]+[kk[i][j]-kk[i][0] for j in range(1,20)]\n",
    "        season.append(kk2)\n",
    "    stds=[np.std([season[0][i],season[1][i],season[2][i],season[3][i]]) for i in range(20)]\n",
    "    maxs=[np.max([season[0][i],season[1][i],season[2][i],season[3][i]]) for i in range(20)]\n",
    "    mins=[np.min([season[0][i],season[1][i],season[2][i],season[3][i]]) for i in range(20)]\n",
    "    kk2=get_01_all(s,t,landcover_type,snowfree=snowfree)\n",
    "    for y in tqdm(range(2002,2021)):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=1]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=1)\n",
    "        if snowfree:\n",
    "            data_m[:,:,:,-1,:]=0\n",
    "            data_m[:,:,:,:,-1]=0\n",
    "        z=albedo_all[:,y-2001].transpose((0,2,3,1)).reshape((12,180,360,1,lc_len+1))*data_m\n",
    "        z[np.isnan(z)]=0\n",
    "        z=z*(mt.reshape(12,1,1,1,1)/365/24/3600)\n",
    "        kk2.append(np.nansum(z[:,s:t])/np.nansum(np.nanmean(data_m[:,s:t],axis=0)))\n",
    "    kk=[0]+[kk2[j]-kk2[0] for j in range(1,len(kk2))]\n",
    "    return kk2,stds,maxs,mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the albedo change curve and seasonal variance.\n",
    "mt=[]\n",
    "for month in range(1,13):\n",
    "    now=datetime.datetime.strptime(f'2001-{month}-1', '%Y-%m-%d')\n",
    "    month_time=(now+relativedelta(months=1)-now).total_seconds()\n",
    "    mt.append(month_time)\n",
    "mt=np.array(mt)\n",
    "data=torch.load('../data/curve.pth')\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_{landcover_type}.pth')\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    albedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            albedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "            mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "\n",
    "    albedo_all[albedo_all==0]=np.nan\n",
    "    albedo_all[mask>=1]=np.nan\n",
    "    area_all=month_fill(area_all,mask>=1)\n",
    "    albedo_all=month_fill(albedo_all,mask>=1)  \n",
    "\n",
    "\n",
    "    kk2,stds,maxs,mins=get_curve(0,180,False)\n",
    "    data[f'G_snow_{landcover_type}']=kk2\n",
    "    data[f'G_snow_{landcover_type}_std']=stds\n",
    "    data[f'G_snow_{landcover_type}_max']=maxs\n",
    "    data[f'G_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(0,180,True)\n",
    "    data[f'G_snowfree_{landcover_type}']=kk2\n",
    "    data[f'G_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'G_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'G_snowfree_{landcover_type}_min']=mins\n",
    "\n",
    "    kk2,stds,maxs,mins=get_curve(0,24,False)\n",
    "    data[f'N_snow_{landcover_type}']=kk2\n",
    "    data[f'N_snow_{landcover_type}_std']=stds\n",
    "    data[f'N_snow_{landcover_type}_max']=maxs\n",
    "    data[f'N_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(-24,180,False)\n",
    "    data[f'S_snow_{landcover_type}']=kk2\n",
    "    data[f'S_snow_{landcover_type}_std']=stds\n",
    "    data[f'S_snow_{landcover_type}_max']=maxs\n",
    "    data[f'S_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(24,90-23,False)\n",
    "    data[f'TN_snow_{landcover_type}']=kk2\n",
    "    data[f'TN_snow_{landcover_type}_std']=stds\n",
    "    data[f'TN_snow_{landcover_type}_max']=maxs\n",
    "    data[f'TN_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(24,90-23,True)\n",
    "    data[f'TN_snowfree_{landcover_type}']=kk2\n",
    "    data[f'TN_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'TN_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'TN_snowfree_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90-23,90+24,False)\n",
    "    data[f'T_snow_{landcover_type}']=kk2\n",
    "    data[f'T_snow_{landcover_type}_std']=stds\n",
    "    data[f'T_snow_{landcover_type}_max']=maxs\n",
    "    data[f'T_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90-23,90+24,True)\n",
    "    data[f'T_snowfree_{landcover_type}']=kk2\n",
    "    data[f'T_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'T_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'T_snowfree_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90+24,-24,False)\n",
    "    data[f'TS_snow_{landcover_type}']=kk2\n",
    "    data[f'TS_snow_{landcover_type}_std']=stds\n",
    "    data[f'TS_snow_{landcover_type}_max']=maxs\n",
    "    data[f'TS_snow_{landcover_type}_min']=mins\n",
    "    \n",
    "    kk2,stds,maxs,mins=get_curve(90+24,-24,True)\n",
    "    data[f'TS_snowfree_{landcover_type}']=kk2\n",
    "    data[f'TS_snowfree_{landcover_type}_std']=stds\n",
    "    data[f'TS_snowfree_{landcover_type}_max']=maxs\n",
    "    data[f'TS_snowfree_{landcover_type}_min']=mins\n",
    "\n",
    "\n",
    "torch.save(data,'../data/curve.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate albedo-multiply-area-multiply-time\n",
    "def get_aat(landcover_type):\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_{landcover_type}.pth')\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    bluealbedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            bluealbedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "            mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "\n",
    "    bluealbedo_all[np.isnan(bluealbedo_all)]=0\n",
    "    bluealbedo_all[mask>=2]=0\n",
    "    area_all[np.isnan(area_all)]=0\n",
    "    area_all[mask>=2]=0\n",
    "    kka=[]\n",
    "    kka.append(np.zeros((12,180, 360, lc_len+1, lc_len+1)))\n",
    "    for y in tqdm(range(2002,2021)):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=2]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=2)\n",
    "        data_y=np.nanmean(data_m,axis=0)\n",
    "        transf=np.nansum(data_y,axis=0)\n",
    "        transf=np.nansum(transf,axis=0)\n",
    "        dif_al=[]\n",
    "        for m in range(12):\n",
    "            exp1_2020=np.expand_dims(bluealbedo_all[m,y-2001],0).repeat(lc_len+1,axis=0)\n",
    "            exp0_2001=np.expand_dims(bluealbedo_all[m,0],1).repeat(lc_len+1,axis=1)\n",
    "            dif_al_m=(exp1_2020-exp0_2001).transpose((2,3,0,1))*data_m[m]\n",
    "            dif_al.append(dif_al_m)\n",
    "        dif_al=np.stack(dif_al)\n",
    "        kka.append(dif_al)\n",
    "    kka=np.stack(kka)\n",
    "    kka=kka.transpose((0,4,5,1,2,3))\n",
    "    kka2=kka*(mt.reshape((1,1,1,12,1,1)))\n",
    "    np.save(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy',kka2)\n",
    "    \n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    get_aat(landcover_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RF Matrix\n",
    "result2={}\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kka=np.load(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy')\n",
    "    kka=kka[-1]\n",
    "    kernels=torch.load(f\"/data2/hzy/albedo2/kernel/kernels.pth\")\n",
    "    rr=[]\n",
    "    for k in tqdm(['HadGEM2', 'HadGEM3', 'CAM5', 'ECHAM6', 'ERAI', 'ERA5']):\n",
    "        kka2=kka*kernels[k]\n",
    "        rr.append(np.nansum(kka2)/365/24/3600/5.1e8)\n",
    "        result2[f'{landcover_type}_{k}']=np.nansum(kka2.reshape((lc_len+1,lc_len+1,-1)),axis=-1)/365/24/3600/5.1e8\n",
    "torch.save(result2,f'../data/RF_ConfusionMatrix_2020.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the RF curve for six different kernels.\n",
    "for is_snowfree in ['snowdynamic','snowfree','snow']:\n",
    "    kernels=torch.load(f\"/data2/hzy/albedo2/kernel/kernels.pth\")\n",
    "    result={}\n",
    "    for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "        kka=np.load(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy')\n",
    "        if is_snowfree=='snowdynamic':\n",
    "            kka[:,:-1,:-1]=0\n",
    "        elif is_snowfree=='snowfree':\n",
    "            kka[:,-1,:]=0\n",
    "            kka[:,:,-1]=0\n",
    "        elif is_snowfree=='snow':\n",
    "            pass\n",
    "        else:\n",
    "            raise()\n",
    "        for key in ['HadGEM2', 'HadGEM3', 'CAM5', 'ECHAM6', 'ERAI', 'ERA5']:\n",
    "            print(f'{landcover_type}_{key}')\n",
    "            kka2=kka*kernels[key]*1e6\n",
    "            kka2[np.isnan(kka)]=0\n",
    "            kk2=[]\n",
    "            for y in range(2001,2021):\n",
    "                ay=np.nansum(kka2[y-2001,:,:])\n",
    "                kk2.append(ay)\n",
    "            kk=[0]+[kk2[j]-kk2[0] for j in range(1,len(kk2))]\n",
    "            result[f'{landcover_type}_{key}']=kk\n",
    "            # print('integrated RF',np.array(kk).sum())\n",
    "            del kka2\n",
    "            gc.collect()\n",
    "            torch.save(result,f'curve_energy_{is_snowfree}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the albedo change map induce different factor change.\n",
    "def get_albedo_change(landcover_type,mode,ismask):\n",
    "    def filter_mode(data,mode,lc_len):\n",
    "        if mode=='snowfree':\n",
    "            data[:,:,:,-1,:]=0\n",
    "            data[:,:,:,:,-1]=0\n",
    "        elif mode=='snowfree-change':\n",
    "            data[:,:,:,-1,:]=0\n",
    "            data[:,:,:,:,-1]=0\n",
    "        elif mode=='all':\n",
    "            pass\n",
    "        elif mode=='snow':\n",
    "            data[:,:,:,:-1,:-1]=0\n",
    "        elif mode=='conversion':\n",
    "            data[:,:,:,-1,:]=0\n",
    "            data[:,:,:,:,-1]=0\n",
    "            for i in range(lc_len+1):\n",
    "                data[:,:,:,i,i]=0       \n",
    "        elif mode=='non_conversion':\n",
    "            mask_filter=np.zeros((lc_len+1,lc_len+1))\n",
    "            for i in range(lc_len):\n",
    "                mask_filter[i,i]=1\n",
    "            data=data*(mask_filter.reshape((1,1,1,lc_len+1,lc_len+1)))\n",
    "        else:\n",
    "            raise('ERROR MODE')\n",
    "        return data\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kk=[0]\n",
    "    kk_map=[]\n",
    "    kk_map.append(np.zeros((180,360)))\n",
    "    for y in range(2002,2021):\n",
    "        data_m=torch.load(f\"/data2/hzy/albedo2/confuse_matrix/confuse_matrix_{y}_{landcover_type}.pth\")\n",
    "        mask_t=np.expand_dims(mask[:,0,:],0).repeat(lc_len+1,axis=0).transpose((1,3,4,0,2))\n",
    "        data_m[mask_t>=1]=np.nan\n",
    "        data_m=month_fill(data_m,mask_t>=1)\n",
    "        data_m2=data_m.copy()\n",
    "        data_m=filter_mode(data_m,mode,lc_len)\n",
    "        dif_al=[]\n",
    "        calibration=(y-2001)/10*0.007\n",
    "        for m in range(12):\n",
    "            exp1_2020=np.expand_dims(albedo_all[m,y-2001],0).repeat(lc_len+1,axis=0)\n",
    "            exp0_2001=np.expand_dims(albedo_all[m,0],1).repeat(lc_len+1,axis=1)\n",
    "            dif_al_m=(exp1_2020-exp0_2001).transpose((2,3,0,1))*data_m[m]\n",
    "            if ismask:\n",
    "                mask_dif=np.abs((exp1_2020-exp0_2001)/exp0_2001).transpose((2,3,0,1))\n",
    "                dif_al_m[mask_dif<calibration]=0\n",
    "            dif_al.append(dif_al_m*mt[m]/365/24/3600)\n",
    "        dif_al=np.stack(dif_al)\n",
    "        dif_al=np.nansum(dif_al,axis=-1)\n",
    "        dif_al=np.nansum(dif_al,axis=-1)\n",
    "        dif_al=np.nansum(dif_al,axis=0)\n",
    "        if not mode=='snowfree-change':\n",
    "            data_m=data_m2\n",
    "            area_tf=np.nansum(data_m,axis=-1)\n",
    "            area_tf=np.nansum(area_tf,axis=-1)\n",
    "            area_tf=np.nanmean(area_tf,axis=0)\n",
    "            kk.append(kk[0]+np.nansum(dif_al)/np.nansum(area_tf))\n",
    "        else:\n",
    "            area_tf=np.nansum(data_m,axis=-1)\n",
    "            area_tf=np.nansum(area_tf,axis=-1)\n",
    "            area_tf=np.nanmean(area_tf,axis=0)\n",
    "            area_tf2=np.nansum(data_m2,axis=-1)\n",
    "            area_tf2=np.nansum(area_tf2,axis=-1)\n",
    "            area_tf2=np.nanmean(area_tf2,axis=0)\n",
    "            snowfree_fraction=area_tf/area_tf2\n",
    "            kk.append(kk[0]+np.nansum(dif_al)/np.nansum(area_tf))\n",
    "            dif_al[snowfree_fraction<0.1]=0\n",
    "        kk_map.append(dif_al/area_tf)\n",
    "    kk_map=np.stack(kk_map)\n",
    "    print(kk)\n",
    "    plt.imshow(kk_map[-1],vmax=0.05,vmin=-0.05,cmap='seismic')\n",
    "    plt.show()\n",
    "    return kk,kk_map\n",
    "    \n",
    "\n",
    "result=torch.load('../data/albedo_change.pth')\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-70_band-shortwave_{landcover_type}.pth')\n",
    "    _,sza_L=torch.load(root_path+'sza_and_szaL.pth')\n",
    "    albedo_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            albedo_all[m-1,y-2001]=stat[f'{y}-{m}-albedo']\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "            mask[m-1,y-2001]=np.expand_dims(sza_L[y-2001,m-1],0).repeat(lc_len+1,axis=0)\n",
    "    albedo_all[albedo_all==0]=np.nan\n",
    "    albedo_all[mask>=1]=np.nan\n",
    "    area_all=month_fill(area_all,mask>=1)\n",
    "    albedo_all=month_fill(albedo_all,mask>=1)\n",
    "    for mode in ['snowfree-change','snowfree','all','snow','conversion','non_conversion']:\n",
    "        print(mode)\n",
    "        result[f'{landcover_type}_{mode}_s']=get_albedo_change(landcover_type,mode,False)\n",
    "    result[f'{landcover_type}_all_c']=get_albedo_change(landcover_type,'all',True)\n",
    "    result[f'{landcover_type}_snowfree-change_c']=get_albedo_change(landcover_type,'snowfree-change',True)\n",
    "torch.save(result,'../data/albedo_change.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the RF map induce different factor change.\n",
    "def get_RF_map(mode,landcover_type):\n",
    "    print(landcover_type,mode)\n",
    "    def filter_mode(data,mode,lc_len):\n",
    "        if mode=='snowfree':\n",
    "            data[:,-1,:]=0\n",
    "            data[:,:,-1]=0\n",
    "        elif mode=='all':\n",
    "            pass\n",
    "        elif mode=='snow':\n",
    "            data[:,:-1,:-1]=0\n",
    "        elif mode=='conversion':\n",
    "            data[:,-1,:]=0\n",
    "            data[:,:,-1]=0\n",
    "            for i in range(lc_len+1):\n",
    "                data[:,i,i]=0       \n",
    "        elif mode=='non_conversion':\n",
    "            mask_filter=np.zeros((1,lc_len+1,lc_len+1))\n",
    "            for i in range(lc_len):\n",
    "                mask_filter[:,i,i]=1\n",
    "            data=data*(mask_filter.reshape((1,lc_len+1,lc_len+1,1,1)))\n",
    "        else:\n",
    "            raise('ERROR MODE')\n",
    "        return data\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    kka2=filter_mode(kka.copy(),mode,lc_len)\n",
    "    kka2=np.sum(kka2,axis=1)\n",
    "    output=np.sum(kka2,axis=1)/365/24/3600/1e6/area_all\n",
    "    return output\n",
    "\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3']:\n",
    "    result={}\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    mt=[]\n",
    "    for month in range(1,13):\n",
    "        now=datetime.datetime.strptime(f'2001-{month}-1', '%Y-%m-%d')\n",
    "        month_time=(now+relativedelta(months=1)-now).total_seconds()\n",
    "        mt.append(month_time)\n",
    "    mt=np.array(mt)\n",
    "    kka=np.load(f'/data2/hzy/albedo2/data/confusematrix_albedo-multiply-area-multiply-time_{landcover_type}.npy')\n",
    "    kernels=torch.load(f\"/data2/hzy/albedo2/kernel/kernels_6.pth\")\n",
    "    kka=kka*kernels['mean']*1e6\n",
    "    kka[np.isnan(kka)]=0\n",
    "    kka=np.sum(kka,axis=3)\n",
    "    area_all=np.zeros((12,20,lc_len+1,180,360)).astype(np.float32)\n",
    "    mask=np.zeros((12,20,lc_len+1,180,360))\n",
    "    stat=torch.load(f'/data2/hzy/albedo2/data/stat_sw-mean_sza-85_band-shortwave_{landcover_type}.pth')\n",
    "    for y in range(2001,2021):\n",
    "        for m in range(1,13):\n",
    "            area_all[m-1,y-2001]=stat[f'{y}-{m}-typearea']\n",
    "    area_all=month_fill(area_all,mask>=1)\n",
    "    area_all=np.nanmean(area_all,axis=0)\n",
    "    area_all=area_all\n",
    "    area_all=np.nansum(area_all,axis=1)\n",
    "\n",
    "    if landcover_type=='LCCS1':\n",
    "        result['snow']=get_RF_map('snow',landcover_type)\n",
    "        plt.imshow(result['snow'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "        plt.show()\n",
    "    result['snowfree']=get_RF_map('snowfree',landcover_type)\n",
    "    plt.imshow(result['snowfree'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "        \n",
    "    result['all']=get_RF_map('all',landcover_type)\n",
    "    plt.imshow(result['all'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "    \n",
    "    result['conversion']=get_RF_map('conversion',landcover_type)\n",
    "    plt.imshow(result['conversion'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "    \n",
    "    result['non_conversion']=get_RF_map('non_conversion',landcover_type)\n",
    "    plt.imshow(result['non_conversion'][-1],vmax=10,vmin=-10,cmap='seismic')\n",
    "    plt.show()\n",
    "    torch.save(result,f'../data/map_RF_{landcover_type}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the tif files of albedo change map.\n",
    "all=[]\n",
    "all_c=[]\n",
    "snowfree=[]\n",
    "snowfree_change=[]\n",
    "snowfree_change_c=[]\n",
    "snow=[]\n",
    "result=torch.load('../data/albedo_change.pth')\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    for key in ['snow','non_conversion','conversion','all','snowfree','snowfree-change']:\n",
    "        if key=='snow':\n",
    "            snow.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "        elif key in ['non_conversion','conversion']:\n",
    "            kk=result[f'{landcover_type}_{key}_s'][1][-1]\n",
    "            kk[np.isnan(kk)]=0\n",
    "            tif_save(kk,f'../data/albedo_{key}_{landcover_type}_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "        elif key=='all':\n",
    "            all.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "            all_c.append(result[f'{landcover_type}_{key}_c'][1][-1])\n",
    "        elif key=='snowfree':\n",
    "            snowfree.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "        elif key=='snowfree-change':\n",
    "            snowfree_change.append(result[f'{landcover_type}_{key}_s'][1][-1])\n",
    "            snowfree_change_c.append(result[f'{landcover_type}_{key}_c'][1][-1])\n",
    "all=np.nanmean(np.stack(all),axis=0)\n",
    "all[np.isnan(all)]=0\n",
    "tif_save(all,f'../data/albedo_all_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "all_c=np.nanmean(np.stack(all_c),axis=0)\n",
    "all_c[np.isnan(all_c)]=0\n",
    "tif_save(all_c,f'../data/albedo_all_1degree_calibration.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snowfree=np.mean(np.stack(snowfree),axis=0).copy()\n",
    "snowfree[np.isnan(snowfree)]=0\n",
    "tif_save(snowfree,f'../data/albedo_snowfree_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snowfree_change=np.mean(np.stack(snowfree_change),axis=0).copy()\n",
    "snowfree_change[np.isnan(snowfree_change)]=0\n",
    "tif_save(snowfree_change,f'../data/albedo_snowfree-change_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snowfree_change_c=np.mean(np.stack(snowfree_change_c),axis=0).copy()\n",
    "snowfree_change_c[np.isnan(snowfree_change_c)]=0\n",
    "tif_save(snowfree_change_c,f'../data/albedo_snowfree-change_1degree_calibration.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snow=np.mean(np.stack(snow),axis=0).copy()\n",
    "snow[np.isnan(snow)]=0\n",
    "tif_save(snow,f'../data/albedo_snow_1degree_calibration.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the tif files of RF map.\n",
    "snow=[]\n",
    "all=[]\n",
    "snowfree=[]\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    result=torch.load(f'../data/map_RF_{landcover_type}.pth')\n",
    "    for key in result.keys():\n",
    "        if key=='snow':\n",
    "            snow.append(result[key][-1]-result[key][0])\n",
    "        elif key in ['non_conversion','conversion']:\n",
    "            kk=result[key][-1]-result[key][0]\n",
    "            kk[np.isnan(kk)]=0\n",
    "            tif_save(kk,f'../data/RF_{key}-{landcover_type}_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "        elif key=='all':\n",
    "            all.append(result[key][-1]-result[key][0])\n",
    "        elif key=='snowfree':\n",
    "            snowfree.append(result[key][-1]-result[key][0])\n",
    "all=np.nanmean(np.stack(all),axis=0)\n",
    "all[np.isnan(all)]=0\n",
    "tif_save(all,f'../data/RF_all_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snowfree=np.nanmean(np.stack(snowfree),axis=0)\n",
    "snowfree[np.isnan(snowfree)]=0\n",
    "tif_save(snowfree,f'../data/RF_snowfree_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "snow=np.nanmean(np.stack(snow),axis=0)\n",
    "snow[np.isnan(snow)]=0\n",
    "tif_save(snowfree,f'../data/RF_snow_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export albedo trend map and MK test\n",
    "result=torch.load('../data/albedo_change.pth')\n",
    "kk=[]\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    kk.append(result[f'{landcover_type}_snowfree-change_s'][1])\n",
    "zz=np.nanmean(np.stack(kk),axis=0)\n",
    "rr=trend_map(zz.reshape((20,-1)).transpose((1,0)))\n",
    "slope=np.array(rr[1]).reshape(180,360)\n",
    "zz2=zz.reshape((20,60,-1,360)).reshape((20,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((20,60,120,-1))\n",
    "zz2=np.mean(zz2,axis=-1).reshape(20,-1).transpose((1,0))\n",
    "rr=trend_map(zz2)\n",
    "p=np.array(rr[0]).reshape(60,120)\n",
    "p[p==-1]=1\n",
    "p[p<0.05]=0\n",
    "p[p>=0.05]=1\n",
    "p[p==0]=2\n",
    "p-=1\n",
    "tif_save(slope,f'../data/albedo_snowfree-change_trend_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "\n",
    "lat=np.array([88.5-3*i for i in range(60)]).reshape((60,1))*np.ones((1,120))\n",
    "lon=np.array([-178.5+3*i for i in range(120)]).reshape((1,120))*np.ones((60,1))\n",
    "lat[p==0]=0\n",
    "lon[p==0]=0\n",
    "lat=list(lat.reshape((-1,1)))\n",
    "lon=list(lon.reshape((-1,1)))\n",
    "coords=[[lat[i],lon[i]] for i in range(len(lat)) if lat[i]!=0]\n",
    "coords_geo=[geometry.Point(i[1],i[0]) for i in coords]\n",
    "gpdd=gpd.GeoDataFrame.from_dict({'id':list(range(len(coords_geo))),'geometry':coords_geo})\n",
    "gpdd=gpdd.set_crs(4326)\n",
    "gpdd.to_file('../data/significant_snowfree-change_mannkendall.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export RF trend map and MK test\n",
    "kk=[]\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    result=torch.load(f'../data/map_RF_{landcover_type}.pth')\n",
    "    kk.append(result['all'])\n",
    "zz=np.nanmean(np.stack(kk),axis=0)\n",
    "rr=trend_map(zz.reshape((20,-1)).transpose((1,0)))\n",
    "slope=np.array(rr[1]).reshape(180,360)\n",
    "zz2=zz.reshape((20,60,-1,360)).reshape((20,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((20,60,120,-1))\n",
    "zz2=np.mean(zz2,axis=-1).reshape(20,-1).transpose((1,0))\n",
    "rr=trend_map(zz2)\n",
    "p=np.array(rr[0]).reshape(60,120)\n",
    "p[p==-1]=1\n",
    "p[p<0.05]=0\n",
    "p[p>=0.05]=1\n",
    "p[p==0]=2\n",
    "p-=1\n",
    "tif_save(slope,f'../data/RF_all_trend_1degree.tif',(-179.5,1,0,89.5,0,-1),p='4326')\n",
    "\n",
    "lat=np.array([88.5-3*i for i in range(60)]).reshape((60,1))*np.ones((1,120))\n",
    "lon=np.array([-178.5+3*i for i in range(120)]).reshape((1,120))*np.ones((60,1))\n",
    "lat[p==0]=0\n",
    "lon[p==0]=0\n",
    "lat=list(lat.reshape((-1,1)))\n",
    "lon=list(lon.reshape((-1,1)))\n",
    "coords=[[lat[i],lon[i]] for i in range(len(lat)) if lat[i]!=0]\n",
    "coords_geo=[geometry.Point(i[1],i[0]) for i in coords]\n",
    "gpdd=gpd.GeoDataFrame.from_dict({'id':list(range(len(coords_geo))),'geometry':coords_geo})\n",
    "gpdd=gpdd.set_crs(4326)\n",
    "gpdd.to_file('../data/RF_significant_all_mannkendall.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two-sided t test for albedo\n",
    "kk=[]\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    kk.append(result[f'{landcover_type}_all_s'][1])\n",
    "zz=np.nanmean(np.stack(kk),axis=0)\n",
    "zz_a2=np.stack(zz[:10]).reshape((10,60,-1,360)).reshape((10,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((10,60,120,-1))\n",
    "zz_a2=np.mean(zz_a2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "zz_b2=np.stack(zz[10:]).reshape((10,60,-1,360)).reshape((10,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((10,60,120,-1))\n",
    "zz_b2=np.mean(zz_b2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "t2,p2=t_test(zz_a2,zz_b2)\n",
    "t_array2=np.array(t2).reshape(60,120)\n",
    "p_array2=np.array(p2).reshape(60,120)\n",
    "lat=np.array([88.5-3*i for i in range(60)]).reshape((60,1))*np.ones((1,120))\n",
    "lon=np.array([-178.5+3*i for i in range(120)]).reshape((1,120))*np.ones((60,1))\n",
    "lat[p_array2==0]=0\n",
    "lon[p_array2==0]=0\n",
    "lat=list(lat.reshape((-1,1)))\n",
    "lon=list(lon.reshape((-1,1)))\n",
    "coords=[[lat[i],lon[i]] for i in range(len(lat)) if lat[i]!=0]\n",
    "coords_geo=[geometry.Point(i[1],i[0]) for i in coords]\n",
    "gpdd=gpd.GeoDataFrame.from_dict({'id':list(range(len(coords_geo))),'geometry':coords_geo})\n",
    "gpdd=gpdd.set_crs(4326)\n",
    "gpdd.to_file('../data/significant_all.shp')\n",
    "gpdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>POINT (-31.50000 79.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (100.50000 76.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>POINT (103.50000 76.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>POINT (106.50000 76.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>POINT (-46.50000 73.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>659</td>\n",
       "      <td>POINT (106.50000 -88.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>660</td>\n",
       "      <td>POINT (109.50000 -88.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>661</td>\n",
       "      <td>POINT (172.50000 -88.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>662</td>\n",
       "      <td>POINT (175.50000 -88.50000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>663</td>\n",
       "      <td>POINT (178.50000 -88.50000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>664 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                     geometry\n",
       "0      0   POINT (-31.50000 79.50000)\n",
       "1      1   POINT (100.50000 76.50000)\n",
       "2      2   POINT (103.50000 76.50000)\n",
       "3      3   POINT (106.50000 76.50000)\n",
       "4      4   POINT (-46.50000 73.50000)\n",
       "..   ...                          ...\n",
       "659  659  POINT (106.50000 -88.50000)\n",
       "660  660  POINT (109.50000 -88.50000)\n",
       "661  661  POINT (172.50000 -88.50000)\n",
       "662  662  POINT (175.50000 -88.50000)\n",
       "663  663  POINT (178.50000 -88.50000)\n",
       "\n",
       "[664 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#two-sided t test for RF\n",
    "kk=[]\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    result=torch.load(f'../data/map_RF_{landcover_type}.pth')\n",
    "    kk.append(result['all'])\n",
    "zz=np.nanmean(np.stack(kk),axis=0)\n",
    "zz_a2=np.stack(zz[:10]).reshape((10,60,-1,360)).reshape((10,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((10,60,120,-1))\n",
    "zz_a2=np.mean(zz_a2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "zz_b2=np.stack(zz[10:]).reshape((10,60,-1,360)).reshape((10,60,3,120,-1)).transpose((0,1,3,2,4)).reshape((10,60,120,-1))\n",
    "zz_b2=np.mean(zz_b2,axis=-1).reshape(10,-1).transpose((1,0))\n",
    "t2,p2=t_test(zz_a2,zz_b2)\n",
    "t_array2=np.array(t2).reshape(60,120)\n",
    "p_array2=np.array(p2).reshape(60,120)\n",
    "lat=np.array([88.5-3*i for i in range(60)]).reshape((60,1))*np.ones((1,120))\n",
    "lon=np.array([-178.5+3*i for i in range(120)]).reshape((1,120))*np.ones((60,1))\n",
    "lat[p_array2==0]=0\n",
    "lon[p_array2==0]=0\n",
    "lat=list(lat.reshape((-1,1)))\n",
    "lon=list(lon.reshape((-1,1)))\n",
    "coords=[[lat[i],lon[i]] for i in range(len(lat)) if lat[i]!=0]\n",
    "coords_geo=[geometry.Point(i[1],i[0]) for i in coords]\n",
    "gpdd=gpd.GeoDataFrame.from_dict({'id':list(range(len(coords_geo))),'geometry':coords_geo})\n",
    "gpdd=gpdd.set_crs(4326)\n",
    "gpdd.to_file('../data/RF_significant_all.shp')\n",
    "gpdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export contribution map of PV, NPV and SWC basde on LCCS2\n",
    "landcover_type='LCCS2'\n",
    "result={}\n",
    "lc_len=10\n",
    "k=[]\n",
    "for i in range(40076):\n",
    "    k.append((pi/180.0)*R*R*abs(math.sin((90.00220831593487-pixel_with*i)/180.0*pi) - math.sin((90.00220831593487-pixel_with*(i+1))/180.0*pi)) * pixel_with)\n",
    "lc01=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2001_{landcover_type}_v2.tif').read(1)\n",
    "lc20=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2020_{landcover_type}_v2.tif').read(1)\n",
    "lc01[lc01==100]=20\n",
    "lc01[lc01==0]=20\n",
    "lc20[lc20==100]=20\n",
    "lc20[lc20==0]=20\n",
    "\n",
    "z=np.zeros(lc01.shape)\n",
    "z[lc01==lc20]=1\n",
    "for i in [1,2,20]:\n",
    "    z[lc01==i]=0\n",
    "lc01[lc01==20]=0\n",
    "lc01[lc20==20]=0\n",
    "lc01[lc01!=0]=1\n",
    "landarea=np.array(k).reshape(-1,1)*lc01\n",
    "\n",
    "for b in ['NDVI','SSI','NMDI']:\n",
    "    for key in ['mean']:\n",
    "        print(b)\n",
    "        img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}_{key}_eg_final.tif').read(1)\n",
    "        img[np.isnan(img)]=0\n",
    "        img=z*img\n",
    "        tif_save(img,f'/data2/hzy/ssd_hzy/G3/{b}_{key}_eg_final_map.tif',global_trf,p='4326')\n",
    "for b in ['NDVI','SSI','NMDI']:\n",
    "    print(b)\n",
    "    img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}_final.tif').read(1)\n",
    "    img[np.isnan(img)]=0\n",
    "    img=z*img\n",
    "    tif_save(img,f'/data2/hzy/ssd_hzy/G3/{b}_final_map.tif',global_trf,p='4326')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly contribution of PV, NPV and SWC to GLMA\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    k=[]\n",
    "    for i in range(40076):\n",
    "        k.append((pi/180.0)*R*R*abs(math.sin((90.00220831593487-pixel_with*i)/180.0*pi) - math.sin((90.00220831593487-pixel_with*(i+1))/180.0*pi)) * pixel_with)\n",
    "    lc01=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2001_{landcover_type}_v2.tif').read(1)\n",
    "    lc20=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2020_{landcover_type}_v2.tif').read(1)\n",
    "    lc01[lc01==100]=20\n",
    "    lc01[lc01==0]=20\n",
    "    lc20[lc20==100]=20\n",
    "    lc20[lc20==0]=20\n",
    "    z=np.zeros(lc01.shape)\n",
    "    z[lc01==lc20]=1\n",
    "    if landcover_type=='IGBP':\n",
    "        for i in [2,16,20]:\n",
    "            z[lc01==i]=0\n",
    "    else:\n",
    "        for i in [1,2,20]:\n",
    "            z[lc01==i]=0\n",
    "    lc01[lc01==20]=0\n",
    "    lc01[lc20==20]=0\n",
    "    lc01[lc01!=0]=1\n",
    "    landarea=np.array(k).reshape(-1,1)*lc01\n",
    "    del lc01,lc20,k\n",
    "    gc.collect()\n",
    "    print(landarea.sum())\n",
    "    ddd={}\n",
    "    for b in ['NDVI','SSI','NMDI']:\n",
    "        print(b)\n",
    "        ddd[b]=[]\n",
    "        for m in tqdm(range(1,13)):\n",
    "            img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}{m}_albedo_final.tif').read(1)\n",
    "            img[np.isnan(img)]=0\n",
    "            img=img*z\n",
    "            img=landarea*img\n",
    "            ddd[b].append(img.sum()/landarea.sum())\n",
    "            del img\n",
    "            gc.collect()\n",
    "    torch.save(ddd,f'seasons_model_dict_{landcover_type}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contribution of PV, NPV and SWC to GLMA over different LULC types\n",
    "for landcover_type in ['LCCS1','LCCS2','LCCS3','IGBP']:\n",
    "    result={}\n",
    "    print(landcover_type)\n",
    "    if landcover_type=='LCCS2':\n",
    "        lc_len=10\n",
    "    elif landcover_type=='LCCS3':\n",
    "        lc_len=9\n",
    "    elif landcover_type=='IGBP':\n",
    "        lc_len=16\n",
    "    elif landcover_type=='LCCS1':\n",
    "        lc_len=15 \n",
    "    k=[]\n",
    "    for i in range(40076):\n",
    "        k.append((pi/180.0)*R*R*abs(math.sin((90.00220831593487-pixel_with*i)/180.0*pi) - math.sin((90.00220831593487-pixel_with*(i+1))/180.0*pi)) * pixel_with)\n",
    "        \n",
    "    lc01=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2001_{landcover_type}_v2.tif').read(1)\n",
    "    lc20=rasterio.open(f'/data2/hzy/albedo2/LCCS/LCCS/2020_{landcover_type}_v2.tif').read(1)\n",
    "    lc01[lc01==100]=20\n",
    "    lc01[lc01==0]=20\n",
    "    lc20[lc20==100]=20\n",
    "    lc20[lc20==0]=20\n",
    "\n",
    "    z=np.zeros(lc01.shape)\n",
    "    z[lc01==lc20]=lc01[lc01==lc20]\n",
    "    if landcover_type=='IGBP':\n",
    "        for i in [2,16,20]:\n",
    "            z[lc01==i]=0\n",
    "    else:\n",
    "        for i in [1,2,20]:\n",
    "            z[lc01==i]=0\n",
    "    lc01[lc01==20]=0\n",
    "    lc01[lc20==20]=0\n",
    "    lc01[lc01!=0]=1\n",
    "\n",
    "\n",
    "\n",
    "    landarea=np.array(k).reshape(-1,1)*lc01\n",
    "    del lc01,lc20,k\n",
    "    gc.collect()\n",
    "    print(landarea.sum())\n",
    "\n",
    "    \n",
    "    for b in ['NDVI','SSI','NMDI']:\n",
    "        result_b=[]\n",
    "        print(b)\n",
    "        img=rasterio.open(f'/data2/hzy/ssd_hzy/G3/{b}_final.tif').read(1)\n",
    "        img[np.isnan(img)]=0\n",
    "        img[z==0]=0\n",
    "        img=landarea*img\n",
    "        for i in range(1,lc_len+1):\n",
    "            img2=img.copy()\n",
    "            img2[z!=i]=0\n",
    "            zz=img2.sum()/landarea.sum()\n",
    "            result_b.append(zz)\n",
    "            print(type_code[landcover_type][i-1],zz)\n",
    "            \n",
    "            del img2\n",
    "            gc.collect()\n",
    "        result[b]=result_b\n",
    "        del img\n",
    "        gc.collect()\n",
    "    \n",
    "    del z\n",
    "    gc.collect()\n",
    "    torch.save(result,f'type_model_dict_{landcover_type}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonhzy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
